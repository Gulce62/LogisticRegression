{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    # Read the data with its path location\n",
    "    try:\n",
    "        data = pd.read_csv(file_name)\n",
    "        return data\n",
    "    except Exception:\n",
    "        sys.exit(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def trainValTestSplit(data):\n",
    "    # Split the data into train, validation, test\n",
    "    shuffled = data.sample(frac=1, random_state=1)\n",
    "    dataSize = len(shuffled)\n",
    "    train = shuffled[:int(dataSize * 0.7)]\n",
    "    val = shuffled[int(dataSize * 0.7):int(dataSize * 0.8)]\n",
    "    test = shuffled[int(dataSize * 0.8):]\n",
    "    return train, val, test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def normalize(X, min, max):\n",
    "    # Normalize the features\n",
    "    X = (X - min) / (max - min)\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_data(file_location):\n",
    "    # Get X_train, X_val, X_test, y_train, y_val, y_test data\n",
    "    data = read_data(file_location)\n",
    "\n",
    "    train, val, test = trainValTestSplit(data)\n",
    "    minVal = train.iloc[:, :-1].min()\n",
    "    maxVal = train.iloc[:, :-1].max()\n",
    "\n",
    "    X_train = np.array(normalize(train.iloc[:, :-1], minVal, maxVal))\n",
    "    X_val = np.array(normalize(val.iloc[:, :-1], minVal, maxVal))\n",
    "    X_test = np.array(normalize(test.iloc[:, :-1], minVal, maxVal))\n",
    "\n",
    "    y_train = np.array(train.iloc[:, -1:])\n",
    "    y_val = np.array(val.iloc[:, -1:])\n",
    "    y_test = np.array(test.iloc[:, -1:])\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def getConfusionMatrix(y_true, y_pred):\n",
    "    # Get the confusion matrix\n",
    "    y_true = pd.Categorical(y_true.ravel())\n",
    "    y_pred = pd.Categorical(y_pred.ravel())\n",
    "    confusion_matrix = pd.crosstab(y_true, y_pred, rownames=['Actual'], colnames=['Predicted'], dropna=False)\n",
    "    sn.heatmap(confusion_matrix, cmap=\"Blues\", annot=True)\n",
    "    return confusion_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def getValAccPlot(val_acc, paramaterName):\n",
    "    # Get the accuracy plot for every epoch with validation set\n",
    "    title = 'Validation Accuracy For Every Epoch With Different ' + paramaterName\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.plot(val_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "absolutePath = input('Enter the file location of the dataset: ')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_data(absolutePath)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42000, 12)\n",
      "X_val shape: (6000, 12)\n",
      "X_test shape: (12000, 12)\n",
      "y_train shape: (42000, 1)\n",
      "y_val shape: (6000, 1)\n",
      "y_test shape: (12000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Sigmoid function\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def gaussianInitialization(dimension):\n",
    "    # Initialize weights with gaussian distribution\n",
    "    np.random.seed(50)\n",
    "    p = np.random.normal(loc=0, scale=1, size=(dimension, 1))\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def uniformInitialization(dimension):\n",
    "    # Initialize weights with uniform distribution\n",
    "    np.random.seed(50)\n",
    "    p = np.random.uniform(size=(dimension, 1))\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def zeroInitialization(dimension):\n",
    "    # Initialize weights with zero\n",
    "    p = np.zeros((dimension, 1))\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def getAccuracy(y_true, y_pred):\n",
    "    # Get the accuracy\n",
    "    accuracyBool = (y_true.ravel() == y_pred.ravel())\n",
    "    accuracy = np.count_nonzero(accuracyBool) / accuracyBool.shape[0]\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, epochs=100, learningRate=0.001, batchSize=64, initialization='gaussian'):\n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "        self.initialization = {'gaussian': lambda dimension: gaussianInitialization(dimension),\n",
    "                               'uniform': lambda dimension: uniformInitialization(dimension),\n",
    "                               'zero': lambda dimension: zeroInitialization(dimension)}[initialization]\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        # Train the logistic function\n",
    "        m = X_train.shape[0]\n",
    "        # Initialize weights\n",
    "        w = self.initialization(X_train.shape[1])\n",
    "        b = self.initialization(1)\n",
    "        # Find the iteration number according to batch size\n",
    "        if m % self.batchSize == 0:\n",
    "            iterationNo = m // self.batchSize\n",
    "        else:\n",
    "            iterationNo = m // self.batchSize + 1\n",
    "        val_acc = []\n",
    "        for epoch in range(self.epochs):  # for every epoch\n",
    "            for batch in range(iterationNo):  # for every batch\n",
    "                # Calculate batch indices\n",
    "                startIdx = batch * self.batchSize\n",
    "                endIdx = startIdx + self.batchSize\n",
    "                # The final batch (it cannot be the same size as others)\n",
    "                if batch == iterationNo - 1:\n",
    "                    prob = sigmoid(np.dot(X_train[startIdx:], w) + b)\n",
    "                    dw = np.dot(X_train[startIdx:].T, (prob - y_train[startIdx:]))\n",
    "                    db = np.sum(prob - y_train[startIdx:])\n",
    "                # All other batches\n",
    "                else:\n",
    "                    prob = sigmoid(np.dot(X_train[startIdx:endIdx], w) + b)\n",
    "                    dw = np.dot(X_train[startIdx:endIdx].T, (prob - y_train[startIdx:endIdx]))\n",
    "                    db = np.sum(prob - y_train[startIdx:endIdx])\n",
    "                # Update rule\n",
    "                w -= self.learningRate * dw\n",
    "                b -= self.learningRate * db\n",
    "            # Calculate validation accuracy for every epoch\n",
    "            y_pred = self.predict(w, b, X_val, 0.5)\n",
    "            val_accuracy = getAccuracy(y_pred, y_val)\n",
    "            val_acc.append(val_accuracy)\n",
    "            print('Epoch', epoch, 'finished. Accuracy is', val_accuracy)\n",
    "        return w, b, val_acc\n",
    "\n",
    "    def predict(self, w, b, X, threshold):\n",
    "        # Predict the examples with logistic function and founded weights\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        prob = sigmoid(np.dot(X, w) + b)\n",
    "        for i in range(prob.shape[0]):\n",
    "            if prob[i, 0] > threshold:\n",
    "                y_pred[i] = 1\n",
    "            else:\n",
    "                y_pred[i] = 0\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training for batch size = 42000 \n",
      "\n",
      "Epoch 0 finished. Accuracy is 0.374\n",
      "Epoch 1 finished. Accuracy is 0.626\n",
      "Epoch 2 finished. Accuracy is 0.626\n",
      "Epoch 3 finished. Accuracy is 0.374\n",
      "Epoch 4 finished. Accuracy is 0.626\n",
      "Epoch 5 finished. Accuracy is 0.6263333333333333\n",
      "Epoch 6 finished. Accuracy is 0.374\n",
      "Epoch 7 finished. Accuracy is 0.626\n",
      "Epoch 8 finished. Accuracy is 0.6316666666666667\n",
      "Epoch 9 finished. Accuracy is 0.374\n",
      "Epoch 10 finished. Accuracy is 0.626\n",
      "Epoch 11 finished. Accuracy is 0.6365\n",
      "Epoch 12 finished. Accuracy is 0.374\n",
      "Epoch 13 finished. Accuracy is 0.626\n",
      "Epoch 14 finished. Accuracy is 0.6401666666666667\n",
      "Epoch 15 finished. Accuracy is 0.374\n",
      "Epoch 16 finished. Accuracy is 0.626\n",
      "Epoch 17 finished. Accuracy is 0.6408333333333334\n",
      "Epoch 18 finished. Accuracy is 0.37683333333333335\n",
      "Epoch 19 finished. Accuracy is 0.626\n",
      "Epoch 20 finished. Accuracy is 0.643\n",
      "Epoch 21 finished. Accuracy is 0.4031666666666667\n",
      "Epoch 22 finished. Accuracy is 0.626\n",
      "Epoch 23 finished. Accuracy is 0.6583333333333333\n",
      "Epoch 24 finished. Accuracy is 0.42633333333333334\n",
      "Epoch 25 finished. Accuracy is 0.626\n",
      "Epoch 26 finished. Accuracy is 0.6953333333333334\n",
      "Epoch 27 finished. Accuracy is 0.45066666666666666\n",
      "Epoch 28 finished. Accuracy is 0.626\n",
      "Epoch 29 finished. Accuracy is 0.7348333333333333\n",
      "Epoch 30 finished. Accuracy is 0.503\n",
      "Epoch 31 finished. Accuracy is 0.626\n",
      "Epoch 32 finished. Accuracy is 0.7848333333333334\n",
      "Epoch 33 finished. Accuracy is 0.6621666666666667\n",
      "Epoch 34 finished. Accuracy is 0.6388333333333334\n",
      "Epoch 35 finished. Accuracy is 0.6413333333333333\n",
      "Epoch 36 finished. Accuracy is 0.6346666666666667\n",
      "Epoch 37 finished. Accuracy is 0.6978333333333333\n",
      "Epoch 38 finished. Accuracy is 0.6598333333333334\n",
      "Epoch 39 finished. Accuracy is 0.5745\n",
      "Epoch 40 finished. Accuracy is 0.6291666666666667\n",
      "Epoch 41 finished. Accuracy is 0.7853333333333333\n",
      "Epoch 42 finished. Accuracy is 0.7916666666666666\n",
      "Epoch 43 finished. Accuracy is 0.6908333333333333\n",
      "Epoch 44 finished. Accuracy is 0.6566666666666666\n",
      "Epoch 45 finished. Accuracy is 0.6036666666666667\n",
      "Epoch 46 finished. Accuracy is 0.6331666666666667\n",
      "Epoch 47 finished. Accuracy is 0.7605\n",
      "Epoch 48 finished. Accuracy is 0.7445\n",
      "Epoch 49 finished. Accuracy is 0.5646666666666667\n",
      "Epoch 50 finished. Accuracy is 0.6315\n",
      "Epoch 51 finished. Accuracy is 0.7921666666666667\n",
      "Epoch 52 finished. Accuracy is 0.8046666666666666\n",
      "Epoch 53 finished. Accuracy is 0.7308333333333333\n",
      "Epoch 54 finished. Accuracy is 0.7066666666666667\n",
      "Epoch 55 finished. Accuracy is 0.5685\n",
      "Epoch 56 finished. Accuracy is 0.6323333333333333\n",
      "Epoch 57 finished. Accuracy is 0.7918333333333333\n",
      "Epoch 58 finished. Accuracy is 0.8023333333333333\n",
      "Epoch 59 finished. Accuracy is 0.7251666666666666\n",
      "Epoch 60 finished. Accuracy is 0.7046666666666667\n",
      "Epoch 61 finished. Accuracy is 0.5831666666666667\n",
      "Epoch 62 finished. Accuracy is 0.6348333333333334\n",
      "Epoch 63 finished. Accuracy is 0.7798333333333334\n",
      "Epoch 64 finished. Accuracy is 0.7846666666666666\n",
      "Epoch 65 finished. Accuracy is 0.6693333333333333\n",
      "Epoch 66 finished. Accuracy is 0.6591666666666667\n",
      "Epoch 67 finished. Accuracy is 0.6766666666666666\n",
      "Epoch 68 finished. Accuracy is 0.6645\n",
      "Epoch 69 finished. Accuracy is 0.671\n",
      "Epoch 70 finished. Accuracy is 0.663\n",
      "Epoch 71 finished. Accuracy is 0.684\n",
      "Epoch 72 finished. Accuracy is 0.6713333333333333\n",
      "Epoch 73 finished. Accuracy is 0.6711666666666667\n",
      "Epoch 74 finished. Accuracy is 0.6678333333333333\n",
      "Epoch 75 finished. Accuracy is 0.6906666666666667\n",
      "Epoch 76 finished. Accuracy is 0.6845\n",
      "Epoch 77 finished. Accuracy is 0.6725\n",
      "Epoch 78 finished. Accuracy is 0.6716666666666666\n",
      "Epoch 79 finished. Accuracy is 0.695\n",
      "Epoch 80 finished. Accuracy is 0.6933333333333334\n",
      "Epoch 81 finished. Accuracy is 0.6761666666666667\n",
      "Epoch 82 finished. Accuracy is 0.6818333333333333\n",
      "Epoch 83 finished. Accuracy is 0.6953333333333334\n",
      "Epoch 84 finished. Accuracy is 0.6981666666666667\n",
      "Epoch 85 finished. Accuracy is 0.6826666666666666\n",
      "Epoch 86 finished. Accuracy is 0.6896666666666667\n",
      "Epoch 87 finished. Accuracy is 0.696\n",
      "Epoch 88 finished. Accuracy is 0.7008333333333333\n",
      "Epoch 89 finished. Accuracy is 0.689\n",
      "Epoch 90 finished. Accuracy is 0.6971666666666667\n",
      "Epoch 91 finished. Accuracy is 0.6963333333333334\n",
      "Epoch 92 finished. Accuracy is 0.703\n",
      "Epoch 93 finished. Accuracy is 0.6926666666666667\n",
      "Epoch 94 finished. Accuracy is 0.7011666666666667\n",
      "Epoch 95 finished. Accuracy is 0.697\n",
      "Epoch 96 finished. Accuracy is 0.7066666666666667\n",
      "Epoch 97 finished. Accuracy is 0.696\n",
      "Epoch 98 finished. Accuracy is 0.7063333333333334\n",
      "Epoch 99 finished. Accuracy is 0.6985\n",
      "\n",
      "Final accuracy for batch size = 42000 is: 0.6985\n",
      "\n",
      "Start training for batch size = 64 \n",
      "\n",
      "Epoch 0 finished. Accuracy is 0.7311666666666666\n",
      "Epoch 1 finished. Accuracy is 0.7778333333333334\n",
      "Epoch 2 finished. Accuracy is 0.7936666666666666\n",
      "Epoch 3 finished. Accuracy is 0.8026666666666666\n",
      "Epoch 4 finished. Accuracy is 0.8048333333333333\n",
      "Epoch 5 finished. Accuracy is 0.8073333333333333\n",
      "Epoch 6 finished. Accuracy is 0.8075\n",
      "Epoch 7 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 8 finished. Accuracy is 0.81\n",
      "Epoch 9 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 10 finished. Accuracy is 0.8076666666666666\n",
      "Epoch 11 finished. Accuracy is 0.8071666666666667\n",
      "Epoch 12 finished. Accuracy is 0.8081666666666667\n",
      "Epoch 13 finished. Accuracy is 0.8086666666666666\n",
      "Epoch 14 finished. Accuracy is 0.8086666666666666\n",
      "Epoch 15 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 16 finished. Accuracy is 0.8095\n",
      "Epoch 17 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 18 finished. Accuracy is 0.8095\n",
      "Epoch 19 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 20 finished. Accuracy is 0.8095\n",
      "Epoch 21 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 22 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 23 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 24 finished. Accuracy is 0.8101666666666667\n",
      "Epoch 25 finished. Accuracy is 0.8101666666666667\n",
      "Epoch 26 finished. Accuracy is 0.81\n",
      "Epoch 27 finished. Accuracy is 0.8101666666666667\n",
      "Epoch 28 finished. Accuracy is 0.8101666666666667\n",
      "Epoch 29 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 30 finished. Accuracy is 0.8095\n",
      "Epoch 31 finished. Accuracy is 0.8095\n",
      "Epoch 32 finished. Accuracy is 0.8095\n",
      "Epoch 33 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 34 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 35 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 36 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 37 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 38 finished. Accuracy is 0.809\n",
      "Epoch 39 finished. Accuracy is 0.809\n",
      "Epoch 40 finished. Accuracy is 0.809\n",
      "Epoch 41 finished. Accuracy is 0.809\n",
      "Epoch 42 finished. Accuracy is 0.809\n",
      "Epoch 43 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 44 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 45 finished. Accuracy is 0.809\n",
      "Epoch 46 finished. Accuracy is 0.809\n",
      "Epoch 47 finished. Accuracy is 0.809\n",
      "Epoch 48 finished. Accuracy is 0.809\n",
      "Epoch 49 finished. Accuracy is 0.809\n",
      "Epoch 50 finished. Accuracy is 0.809\n",
      "Epoch 51 finished. Accuracy is 0.809\n",
      "Epoch 52 finished. Accuracy is 0.809\n",
      "Epoch 53 finished. Accuracy is 0.809\n",
      "Epoch 54 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 55 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 56 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 57 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 58 finished. Accuracy is 0.809\n",
      "Epoch 59 finished. Accuracy is 0.809\n",
      "Epoch 60 finished. Accuracy is 0.809\n",
      "Epoch 61 finished. Accuracy is 0.809\n",
      "Epoch 62 finished. Accuracy is 0.809\n",
      "Epoch 63 finished. Accuracy is 0.809\n",
      "Epoch 64 finished. Accuracy is 0.809\n",
      "Epoch 65 finished. Accuracy is 0.809\n",
      "Epoch 66 finished. Accuracy is 0.809\n",
      "Epoch 67 finished. Accuracy is 0.809\n",
      "Epoch 68 finished. Accuracy is 0.809\n",
      "Epoch 69 finished. Accuracy is 0.809\n",
      "Epoch 70 finished. Accuracy is 0.809\n",
      "Epoch 71 finished. Accuracy is 0.809\n",
      "Epoch 72 finished. Accuracy is 0.809\n",
      "Epoch 73 finished. Accuracy is 0.809\n",
      "Epoch 74 finished. Accuracy is 0.809\n",
      "Epoch 75 finished. Accuracy is 0.809\n",
      "Epoch 76 finished. Accuracy is 0.809\n",
      "Epoch 77 finished. Accuracy is 0.809\n",
      "Epoch 78 finished. Accuracy is 0.809\n",
      "Epoch 79 finished. Accuracy is 0.809\n",
      "Epoch 80 finished. Accuracy is 0.809\n",
      "Epoch 81 finished. Accuracy is 0.809\n",
      "Epoch 82 finished. Accuracy is 0.809\n",
      "Epoch 83 finished. Accuracy is 0.809\n",
      "Epoch 84 finished. Accuracy is 0.809\n",
      "Epoch 85 finished. Accuracy is 0.809\n",
      "Epoch 86 finished. Accuracy is 0.809\n",
      "Epoch 87 finished. Accuracy is 0.809\n",
      "Epoch 88 finished. Accuracy is 0.809\n",
      "Epoch 89 finished. Accuracy is 0.809\n",
      "Epoch 90 finished. Accuracy is 0.809\n",
      "Epoch 91 finished. Accuracy is 0.809\n",
      "Epoch 92 finished. Accuracy is 0.809\n",
      "Epoch 93 finished. Accuracy is 0.809\n",
      "Epoch 94 finished. Accuracy is 0.809\n",
      "Epoch 95 finished. Accuracy is 0.809\n",
      "Epoch 96 finished. Accuracy is 0.809\n",
      "Epoch 97 finished. Accuracy is 0.809\n",
      "Epoch 98 finished. Accuracy is 0.809\n",
      "Epoch 99 finished. Accuracy is 0.809\n",
      "\n",
      "Final accuracy for batch size = 64 is: 0.809\n",
      "\n",
      "Start training for batch size = 1 \n",
      "\n",
      "Epoch 0 finished. Accuracy is 0.7318333333333333\n",
      "Epoch 1 finished. Accuracy is 0.778\n",
      "Epoch 2 finished. Accuracy is 0.7938333333333333\n",
      "Epoch 3 finished. Accuracy is 0.8026666666666666\n",
      "Epoch 4 finished. Accuracy is 0.805\n",
      "Epoch 5 finished. Accuracy is 0.8075\n",
      "Epoch 6 finished. Accuracy is 0.8076666666666666\n",
      "Epoch 7 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 8 finished. Accuracy is 0.8101666666666667\n",
      "Epoch 9 finished. Accuracy is 0.8085\n",
      "Epoch 10 finished. Accuracy is 0.8073333333333333\n",
      "Epoch 11 finished. Accuracy is 0.8071666666666667\n",
      "Epoch 12 finished. Accuracy is 0.8083333333333333\n",
      "Epoch 13 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 14 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 15 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 16 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 17 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 18 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 19 finished. Accuracy is 0.8093333333333333\n",
      "Epoch 20 finished. Accuracy is 0.8096666666666666\n",
      "Epoch 21 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 22 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 23 finished. Accuracy is 0.8098333333333333\n",
      "Epoch 24 finished. Accuracy is 0.81\n",
      "Epoch 25 finished. Accuracy is 0.8101666666666667\n",
      "Epoch 26 finished. Accuracy is 0.81\n",
      "Epoch 27 finished. Accuracy is 0.8101666666666667\n",
      "Epoch 28 finished. Accuracy is 0.81\n",
      "Epoch 29 finished. Accuracy is 0.8095\n",
      "Epoch 30 finished. Accuracy is 0.8095\n",
      "Epoch 31 finished. Accuracy is 0.8095\n",
      "Epoch 32 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 33 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 34 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 35 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 36 finished. Accuracy is 0.809\n",
      "Epoch 37 finished. Accuracy is 0.809\n",
      "Epoch 38 finished. Accuracy is 0.809\n",
      "Epoch 39 finished. Accuracy is 0.809\n",
      "Epoch 40 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 41 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 42 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 43 finished. Accuracy is 0.8088333333333333\n",
      "Epoch 44 finished. Accuracy is 0.809\n",
      "Epoch 45 finished. Accuracy is 0.809\n",
      "Epoch 46 finished. Accuracy is 0.809\n",
      "Epoch 47 finished. Accuracy is 0.809\n",
      "Epoch 48 finished. Accuracy is 0.809\n",
      "Epoch 49 finished. Accuracy is 0.809\n",
      "Epoch 50 finished. Accuracy is 0.809\n",
      "Epoch 51 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 52 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 53 finished. Accuracy is 0.8091666666666667\n",
      "Epoch 54 finished. Accuracy is 0.809\n",
      "Epoch 55 finished. Accuracy is 0.809\n",
      "Epoch 56 finished. Accuracy is 0.809\n",
      "Epoch 57 finished. Accuracy is 0.809\n",
      "Epoch 58 finished. Accuracy is 0.809\n",
      "Epoch 59 finished. Accuracy is 0.809\n",
      "Epoch 60 finished. Accuracy is 0.809\n",
      "Epoch 61 finished. Accuracy is 0.809\n",
      "Epoch 62 finished. Accuracy is 0.809\n",
      "Epoch 63 finished. Accuracy is 0.809\n",
      "Epoch 64 finished. Accuracy is 0.809\n",
      "Epoch 65 finished. Accuracy is 0.809\n",
      "Epoch 66 finished. Accuracy is 0.809\n",
      "Epoch 67 finished. Accuracy is 0.809\n",
      "Epoch 68 finished. Accuracy is 0.809\n",
      "Epoch 69 finished. Accuracy is 0.809\n",
      "Epoch 70 finished. Accuracy is 0.809\n",
      "Epoch 71 finished. Accuracy is 0.809\n",
      "Epoch 72 finished. Accuracy is 0.809\n",
      "Epoch 73 finished. Accuracy is 0.809\n",
      "Epoch 74 finished. Accuracy is 0.809\n",
      "Epoch 75 finished. Accuracy is 0.809\n",
      "Epoch 76 finished. Accuracy is 0.809\n",
      "Epoch 77 finished. Accuracy is 0.809\n",
      "Epoch 78 finished. Accuracy is 0.809\n",
      "Epoch 79 finished. Accuracy is 0.809\n",
      "Epoch 80 finished. Accuracy is 0.809\n",
      "Epoch 81 finished. Accuracy is 0.809\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal batch size\n",
    "batchSize = [42000, 64, 1]\n",
    "batchSizeAcc = []\n",
    "parameterList = []\n",
    "plt.figure()\n",
    "for size in batchSize:\n",
    "    print('\\nStart training for batch size =', size, '\\n')\n",
    "    model = LogisticRegression(epochs=100, learningRate=0.001, batchSize=size, initialization='gaussian')\n",
    "    w, b, val_acc = model.fit(X_train, y_train, X_val, y_val)\n",
    "    parameterList.append((w, b))\n",
    "    getValAccPlot(val_acc, 'Batch Size')\n",
    "    y_pred = model.predict(w, b, X_val, 0.5)\n",
    "    accuracy = getAccuracy(y_pred, y_val)\n",
    "    batchSizeAcc.append(accuracy)\n",
    "    print('\\nFinal accuracy for batch size =', size, 'is:', accuracy)\n",
    "plt.legend(['full-batch: 42000', 'mini-batch: 64', 'stochastic: 1'])\n",
    "plt.show()\n",
    "\n",
    "maxBatchSize = batchSize[np.argmax(batchSizeAcc)]\n",
    "model = LogisticRegression(epochs=100, learningRate=0.001, batchSize=maxBatchSize, initialization='gaussian')\n",
    "w, b = parameterList[np.argmax(batchSizeAcc)]\n",
    "y_pred = model.predict(w, b, X_test, 0.5)\n",
    "title = 'Confusion Matrix for batch size = ' + str(maxBatchSize)\n",
    "plt.title(title)\n",
    "getConfusionMatrix(y_pred, y_test)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the optimal initialization\n",
    "initializationType = ['gaussian', 'uniform', 'zero']\n",
    "initializationAcc = []\n",
    "parameterList = []\n",
    "plt.figure()\n",
    "for inType in initializationType:\n",
    "    print('\\nStart training for', inType, 'initialization\\n')\n",
    "    model = LogisticRegression(epochs=100, learningRate=0.001, batchSize=64, initialization=inType)\n",
    "    w, b, val_acc = model.fit(X_train, y_train, X_val, y_val)\n",
    "    parameterList.append((w, b))\n",
    "    getValAccPlot(val_acc, 'Initialization Type')\n",
    "    y_pred = model.predict(w, b, X_val, 0.5)\n",
    "    accuracy = getAccuracy(y_pred, y_val)\n",
    "    initializationAcc.append(accuracy)\n",
    "    print('\\nFinal accuracy for for', inType, 'initialization is:', accuracy)\n",
    "plt.legend(initializationType)\n",
    "plt.show()\n",
    "\n",
    "maxInitialization = initializationType[np.argmax(initializationAcc)]\n",
    "model = LogisticRegression(epochs=100, learningRate=0.001, batchSize=64, initialization=maxInitialization)\n",
    "w, b = parameterList[np.argmax(initializationAcc)]\n",
    "y_pred = model.predict(w, b, X_test, 0.5)\n",
    "title = 'Confusion Matrix for ' + str(maxInitialization) + ' initialization'\n",
    "plt.title(title)\n",
    "getConfusionMatrix(y_pred, y_test)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the optimal learning rate\n",
    "learningRate = [1, 0.001, 0.0001, 0.00001]\n",
    "learningRateAcc = []\n",
    "parameterList = []\n",
    "plt.figure()\n",
    "for rate in learningRate:\n",
    "    print('\\nStart training for learning rate =', rate, '\\n')\n",
    "    model = LR.LogisticRegression(epochs=100, learningRate=rate, batchSize=64, initialization='gaussian')\n",
    "    w, b, val_acc = model.fit(X_train, y_train, X_val, y_val)\n",
    "    parameterList.append((w, b))\n",
    "    getValAccPlot(val_acc, 'Learning Rate')\n",
    "    y_pred = model.predict(w, b, X_val, 0.5)\n",
    "    accuracy = getAccuracy(y_pred, y_val)\n",
    "    learningRateAcc.append(accuracy)\n",
    "    print('\\nFinal accuracy for learning rate =', rate, 'is:', accuracy)\n",
    "plt.legend(learningRate)\n",
    "plt.show()\n",
    "\n",
    "maxLearningRate = learningRate[np.argmax(learningRateAcc)]\n",
    "model = LogisticRegression(epochs=100, learningRate=maxLearningRate, batchSize=64, initialization='gaussian')\n",
    "w, b = parameterList[np.argmax(learningRateAcc)]\n",
    "y_pred = model.predict(w, b, X_test, 0.5)\n",
    "title = 'Confusion Matrix for learning rate = ' + str(maxLearningRate)\n",
    "plt.title(title)\n",
    "getConfusionMatrix(y_pred, y_test)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Final training with the best hyperparameters\n",
    "print('\\nStart final training with best parameters\\n')\n",
    "print('epochs=100, learningRate=', maxLearningRate, 'batchSize=', maxBatchSize, 'initialization=', maxInitialization)\n",
    "model = LR.LogisticRegression(epochs=100, learningRate=maxLearningRate, batchSize=maxBatchSize,\n",
    "                              initialization=maxInitialization)\n",
    "w, b, val_acc = model.fit(X_train, y_train, X_val, y_val)\n",
    "y_pred = model.predict(w, b, X_test, 0.5)\n",
    "accuracy = getAccuracy(y_pred, y_test)\n",
    "cm = getConfusionMatrix(y_pred, y_test)\n",
    "plt.title('Confusion Matrix For Test Set')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "true_pos = np.diag(cm)[0]\n",
    "false_pos = np.sum(cm, axis=1)[0] - true_pos\n",
    "\n",
    "pre_denom = np.sum(cm, axis=1)[0]\n",
    "rec_denom = np.sum(cm, axis=0)[0]\n",
    "FPR_denom = np.sum(cm, axis=0)[1]\n",
    "\n",
    "precision = true_pos/pre_denom\n",
    "recall = true_pos/rec_denom\n",
    "FPR = false_pos/FPR_denom\n",
    "F1_beta_square = 1**2\n",
    "F1_measure = ((1 + F1_beta_square)*precision*recall)/((F1_beta_square*precision)+recall)\n",
    "F2_beta_square = 2**2\n",
    "F2_measure = ((1 + F2_beta_square)*precision*recall)/((F2_beta_square*precision)+recall)\n",
    "F05_beta_square = 0.5**2\n",
    "F05_measure = ((1 + F05_beta_square)*precision*recall)/((F05_beta_square*precision)+recall)\n",
    "\n",
    "print('\\nAccuracy for test set is:', accuracy)\n",
    "print('Precision for test set is:', precision)\n",
    "print('Recall for test set is:', recall)\n",
    "print('False Positive Rate (FPR) for test set is:', FPR)\n",
    "print('F1 measure for test set is:', F1_measure)\n",
    "print('F2 measure for test set is:', F2_measure)\n",
    "print('F0.5 measure for test set is:', F05_measure)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
