{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model as l"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    # Read the data with its path location\n",
    "    try:\n",
    "        data = pd.read_csv(file_name)\n",
    "        return data\n",
    "    except Exception:\n",
    "        sys.exit(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def trainValTestSplit(data):\n",
    "    shuffled = data.sample(frac=1, random_state=0)\n",
    "    dataSize = len(shuffled)\n",
    "    train = shuffled[:int(dataSize * 0.7)]\n",
    "    val = shuffled[int(dataSize * 0.7):int(dataSize * 0.8)]\n",
    "    test = shuffled[int(dataSize * 0.8):]\n",
    "    return train, val, test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def normalize(X, min, max):\n",
    "    X = (X - min) / (max - min)\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_data(file_location):\n",
    "    data = read_data(file_location)\n",
    "\n",
    "    train, val, test = trainValTestSplit(data)\n",
    "    minVal = train.iloc[:, :-1].min()\n",
    "    maxVal = train.iloc[:, :-1].max()\n",
    "\n",
    "    X_train = np.array(normalize(train.iloc[:, :-1], minVal, maxVal))\n",
    "    X_val = np.array(normalize(val.iloc[:, :-1], minVal, maxVal))\n",
    "    X_test = np.array(normalize(test.iloc[:, :-1], minVal, maxVal))\n",
    "\n",
    "    y_train = np.array(train.iloc[:, -1:])\n",
    "    y_val = np.array(val.iloc[:, -1:])\n",
    "    y_test = np.array(test.iloc[:, -1:])\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "absolutePath = r'C:\\Users\\gulce\\Desktop\\EEE 8TH SEMESTER\\CS 464\\Homeworks\\HW2\\dataset.csv'\n",
    "# absolutePath = input('Enter the file location of the dataset: ')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_data(absolutePath)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42000, 12)\n",
      "X_val shape: (6000, 12)\n",
      "X_test shape: (12000, 12)\n",
      "y_train shape: (42000, 1)\n",
      "y_val shape: (6000, 1)\n",
      "y_test shape: (12000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gulce\\anaconda3\\envs\\LogisticRegression\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8150833333333334\n"
     ]
    }
   ],
   "source": [
    "clf = l.LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def gaussianInitialization(dimension):\n",
    "    np.random.seed(9)\n",
    "    w = np.random.normal(loc=0, scale=1, size=(dimension, 1))\n",
    "    return w"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def zeroInitialization(dimension):\n",
    "    np.random.seed(9)\n",
    "    w = np.zeros((dimension, 1))\n",
    "    return w"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, epochs=100, learningRate=0.001, batchSize=64):\n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        m = X_train.shape[0]\n",
    "        w = zeroInitialization(X_train.shape[1])\n",
    "        b = zeroInitialization(1)\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch in range(m//self.batchSize + 1):\n",
    "                startIdx = batch*self.batchSize\n",
    "                endIdx = 2*batch*self.batchSize\n",
    "                if batch == m//self.batchSize:\n",
    "                    prob = sigmoid(np.dot(X_train[startIdx:endIdx], w) + b)\n",
    "                    dw = (1 / self.batchSize) * np.dot(X_train[startIdx:endIdx].T, (prob - y_train[startIdx:endIdx]))\n",
    "                    db = (1 / self.batchSize) * np.sum(prob - y_train[startIdx:endIdx])\n",
    "                else:\n",
    "                    prob = sigmoid(np.dot(X_train[startIdx:], w) + b)\n",
    "                    dw = (1 / self.batchSize) * np.dot(X_train[startIdx:].T, (prob - y_train[startIdx:]))\n",
    "                    db = (1 / self.batchSize) * np.sum(prob - y_train[batch])\n",
    "                w -= self.learningRate * dw\n",
    "                b -= self.learningRate * db\n",
    "            print('-------------- Epoch', epoch, 'finished --------------')\n",
    "        return w, b\n",
    "\n",
    "    def predict(self, w, b, X, threshold):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        #w = w.reshape(X.shape[1], 1)\n",
    "        prob = sigmoid(np.dot(X, w) + b)\n",
    "        for i in range(prob.shape[0]):\n",
    "            if prob[i,0] > threshold:\n",
    "                y_pred[i] = 1\n",
    "            else:\n",
    "                y_pred[i] = 0\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training for batchsize = 42000 \n",
      "\n",
      "-------------- Epoch 0 finished --------------\n",
      "-------------- Epoch 1 finished --------------\n",
      "-------------- Epoch 2 finished --------------\n",
      "-------------- Epoch 3 finished --------------\n",
      "-------------- Epoch 4 finished --------------\n",
      "-------------- Epoch 5 finished --------------\n",
      "-------------- Epoch 6 finished --------------\n",
      "-------------- Epoch 7 finished --------------\n",
      "-------------- Epoch 8 finished --------------\n",
      "-------------- Epoch 9 finished --------------\n",
      "-------------- Epoch 10 finished --------------\n",
      "-------------- Epoch 11 finished --------------\n",
      "-------------- Epoch 12 finished --------------\n",
      "-------------- Epoch 13 finished --------------\n",
      "-------------- Epoch 14 finished --------------\n",
      "-------------- Epoch 15 finished --------------\n",
      "-------------- Epoch 16 finished --------------\n",
      "-------------- Epoch 17 finished --------------\n",
      "-------------- Epoch 18 finished --------------\n",
      "-------------- Epoch 19 finished --------------\n",
      "-------------- Epoch 20 finished --------------\n",
      "-------------- Epoch 21 finished --------------\n",
      "-------------- Epoch 22 finished --------------\n",
      "-------------- Epoch 23 finished --------------\n",
      "-------------- Epoch 24 finished --------------\n",
      "-------------- Epoch 25 finished --------------\n",
      "-------------- Epoch 26 finished --------------\n",
      "-------------- Epoch 27 finished --------------\n",
      "-------------- Epoch 28 finished --------------\n",
      "-------------- Epoch 29 finished --------------\n",
      "-------------- Epoch 30 finished --------------\n",
      "-------------- Epoch 31 finished --------------\n",
      "-------------- Epoch 32 finished --------------\n",
      "-------------- Epoch 33 finished --------------\n",
      "-------------- Epoch 34 finished --------------\n",
      "-------------- Epoch 35 finished --------------\n",
      "-------------- Epoch 36 finished --------------\n",
      "-------------- Epoch 37 finished --------------\n",
      "-------------- Epoch 38 finished --------------\n",
      "-------------- Epoch 39 finished --------------\n",
      "-------------- Epoch 40 finished --------------\n",
      "-------------- Epoch 41 finished --------------\n",
      "-------------- Epoch 42 finished --------------\n",
      "-------------- Epoch 43 finished --------------\n",
      "-------------- Epoch 44 finished --------------\n",
      "-------------- Epoch 45 finished --------------\n",
      "-------------- Epoch 46 finished --------------\n",
      "-------------- Epoch 47 finished --------------\n",
      "-------------- Epoch 48 finished --------------\n",
      "-------------- Epoch 49 finished --------------\n",
      "-------------- Epoch 50 finished --------------\n",
      "-------------- Epoch 51 finished --------------\n",
      "-------------- Epoch 52 finished --------------\n",
      "-------------- Epoch 53 finished --------------\n",
      "-------------- Epoch 54 finished --------------\n",
      "-------------- Epoch 55 finished --------------\n",
      "-------------- Epoch 56 finished --------------\n",
      "-------------- Epoch 57 finished --------------\n",
      "-------------- Epoch 58 finished --------------\n",
      "-------------- Epoch 59 finished --------------\n",
      "-------------- Epoch 60 finished --------------\n",
      "-------------- Epoch 61 finished --------------\n",
      "-------------- Epoch 62 finished --------------\n",
      "-------------- Epoch 63 finished --------------\n",
      "-------------- Epoch 64 finished --------------\n",
      "-------------- Epoch 65 finished --------------\n",
      "-------------- Epoch 66 finished --------------\n",
      "-------------- Epoch 67 finished --------------\n",
      "-------------- Epoch 68 finished --------------\n",
      "-------------- Epoch 69 finished --------------\n",
      "-------------- Epoch 70 finished --------------\n",
      "-------------- Epoch 71 finished --------------\n",
      "-------------- Epoch 72 finished --------------\n",
      "-------------- Epoch 73 finished --------------\n",
      "-------------- Epoch 74 finished --------------\n",
      "-------------- Epoch 75 finished --------------\n",
      "-------------- Epoch 76 finished --------------\n",
      "-------------- Epoch 77 finished --------------\n",
      "-------------- Epoch 78 finished --------------\n",
      "-------------- Epoch 79 finished --------------\n",
      "-------------- Epoch 80 finished --------------\n",
      "-------------- Epoch 81 finished --------------\n",
      "-------------- Epoch 82 finished --------------\n",
      "-------------- Epoch 83 finished --------------\n",
      "-------------- Epoch 84 finished --------------\n",
      "-------------- Epoch 85 finished --------------\n",
      "-------------- Epoch 86 finished --------------\n",
      "-------------- Epoch 87 finished --------------\n",
      "-------------- Epoch 88 finished --------------\n",
      "-------------- Epoch 89 finished --------------\n",
      "-------------- Epoch 90 finished --------------\n",
      "-------------- Epoch 91 finished --------------\n",
      "-------------- Epoch 92 finished --------------\n",
      "-------------- Epoch 93 finished --------------\n",
      "-------------- Epoch 94 finished --------------\n",
      "-------------- Epoch 95 finished --------------\n",
      "-------------- Epoch 96 finished --------------\n",
      "-------------- Epoch 97 finished --------------\n",
      "-------------- Epoch 98 finished --------------\n",
      "-------------- Epoch 99 finished --------------\n",
      "\n",
      "Acc for batchsize = 42000 is: 0.6366666666666667\n",
      "\n",
      "Start training for batchsize = 64 \n",
      "\n",
      "-------------- Epoch 0 finished --------------\n",
      "-------------- Epoch 1 finished --------------\n",
      "-------------- Epoch 2 finished --------------\n",
      "-------------- Epoch 3 finished --------------\n",
      "-------------- Epoch 4 finished --------------\n",
      "-------------- Epoch 5 finished --------------\n",
      "-------------- Epoch 6 finished --------------\n",
      "-------------- Epoch 7 finished --------------\n",
      "-------------- Epoch 8 finished --------------\n",
      "-------------- Epoch 9 finished --------------\n",
      "-------------- Epoch 10 finished --------------\n",
      "-------------- Epoch 11 finished --------------\n",
      "-------------- Epoch 12 finished --------------\n",
      "-------------- Epoch 13 finished --------------\n",
      "-------------- Epoch 14 finished --------------\n",
      "-------------- Epoch 15 finished --------------\n",
      "-------------- Epoch 16 finished --------------\n",
      "-------------- Epoch 17 finished --------------\n",
      "-------------- Epoch 18 finished --------------\n",
      "-------------- Epoch 19 finished --------------\n",
      "-------------- Epoch 20 finished --------------\n",
      "-------------- Epoch 21 finished --------------\n",
      "-------------- Epoch 22 finished --------------\n",
      "-------------- Epoch 23 finished --------------\n",
      "-------------- Epoch 24 finished --------------\n",
      "-------------- Epoch 25 finished --------------\n"
     ]
    }
   ],
   "source": [
    "batchSize = [42000, 64, 1]\n",
    "acc = []\n",
    "for size in batchSize:\n",
    "    print('\\nStart training for batchsize =', size, '\\n')\n",
    "    model = LogisticRegression(epochs=100, learningRate=0.001, batchSize=size)\n",
    "    w, b = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(w, b, X_val, 0.5)\n",
    "    accuracy = accuracy_score(y_pred, y_val)\n",
    "    acc.append(accuracy)\n",
    "    print('\\nAcc for batchsize =', size, 'is:', accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
